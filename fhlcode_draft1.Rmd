---
title: "FHL_code_draft1"
author: "Jasmine Reighard"
date: "2023-09-20"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(bbmle)
library(cli)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(car)
library(lubridate)
library(ggpubr)
library(tidyverse)
library(glmmTMB)
library(viridis)
library(slider)
library(sjPlot)
library(ggeffects)
library(modelsummary)
library(stats)
library(lme4)
library(agricolae)
library(ggplot2)
library(dplyr)
library(car)
library(MuMIn)
library(fishualize)
library(AER)
library(stringr)
library(corrr)
library(ggcorrplot)
library(FactoMineR)

```


```{r}
#download data

#all data
cym_dat <- read.csv("data/rawdata_allcombinedSep28.csv")

#NND data
nnd_df <- read.csv("data/rawdata_combined_NND_9.28.csv")

#spontaneous turns
spon_turn <- read.csv("data/sp_turn_10-9-23.csv")

#categorize by s and w
cym_dat$s_w_reactor <- ifelse(substr(cym_dat$responder, 1, 1) == "s", "s", "w")


wave_dat <- subset(cym_dat, substr(responder, 1, 1) == "w")

#make sure variables are as they are supposed to be

cym_dat$s_w_reactor <- as.factor(cym_dat$s_w_reactor)
cym_dat$size <- as.factor(cym_dat$size)
cym_dat$school <- as.factor(cym_dat$school)
cym_dat$stimulus <- as.factor(cym_dat$stimulus)



```

```{r}
#Define thresholds for fast and slow responses for small and large

#look at averages
turnrate_result <- aggregate(turning_rate ~ size, data = spon_turn, FUN = mean)
print(turnrate_result)
#large 0.2243052
#small 0.243546

#Mann-Whitney test
manwhit_spon <- wilcox.test(turning_rate ~ size, data = spon_turn)
print(manwhit_spon) 
# p-value is less not less than 0.05, conclusion: there is no statistically significant difference in the distributions of "turning_rate" between the two size.


spon_anova <- aov(turning_rate ~ size, data = spon_turn)
print(summary(spon_anova))

plot(spon_anova)
shapiro.test(spon_anova$residuals)
par(mfrow=c(2,2))
plot(spon_anova)



#homogeneity of variances
leveneTest(turning_rate ~ size, data = spon_turn)

t.test(turning_rate ~ size, data = spon_turn)


avg_turnrate <- mean(spon_turn$turning_rate)
print(avg_turnrate)
#avg turn rate is .2321768
```

```{r}
#calculate turning duration on all data
# Calculate density 

cym_dat_frames <- read.csv("data/rawdata_allcombinedSep28_start_end.csv")
cym_dat_frames <- cym_dat_frames %>% 
  mutate(turning_duration = (frame_end - frame_st)/240 * 1000)

#add turning duration to cym_dat df
cym_dat$turning_duration <- cym_dat_frames$turning_duration

#calculate turning rate
cym_dat <- cym_dat %>% 
  mutate(turning_rate = turning_angle_absolute_value / turning_duration)

#categorize responses

cym_dat$response_type <- ifelse(
  (cym_dat$size == "large" & cym_dat$turning_rate > 0.22) |
  (cym_dat$size == "small" & cym_dat$turning_rate > 0.24),
  "fast",
  "slow"
)

#make sure variables are as they are supposed to be

cym_dat$s_w_reactor <- as.factor(cym_dat$s_w_reactor)
cym_dat$size <- as.factor(cym_dat$size)
cym_dat$school <- as.factor(cym_dat$school)
cym_dat$stimulus <- as.factor(cym_dat$stimulus)
cym_dat$response_type <- as.factor(cym_dat$response_type)
```

```{r}
#visualize data fast and slow responses

#all together
ggplot(cym_dat,aes(x=turning_duration,y=turning_angle_absolute_value,color=size))+
         geom_point()+
         geom_smooth(method=lm,fullrange=TRUE,
                  aes(color=size))+
        scale_color_manual(name='Size',
                     breaks=c('large', 'small'),
                     values=c('large'='#E69F00', 'small'='#0C7BDC'))+
   theme(legend.title=element_text(size=14),
       legend.text=element_text(size=14))+
         theme_classic()




#visualize percent slow
summary_table <- cym_dat %>%
  group_by(size, response_type) %>%
  summarise(count = n()) %>%
  mutate(percentage = count / sum(count) * 100)

ggplot(summary_table, aes(x = size, y = percentage, fill = response_type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Percentage of Fast and Slow Responses by Size Group", y = "Percentage") +
  scale_fill_fish_d(option = "Trimma_lantana") +
  theme_minimal()

#frequency of fast and slow
summary_table <- cym_dat %>%
  group_by(size, response_type) %>%
  summarise(count = n())

ggplot(summary_table, aes(x = size, y = count, fill = response_type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Frequency of Fast and Slow Responses by Size Group", y = "Frequency") +
  scale_fill_fish_d(option = "Trimma_lantana") +
  theme_minimal()


#scatter plot grouped by fast and slow

ggplot(cym_dat,aes(x=turning_duration,y=turning_angle_absolute_value,color=response_type))+
         geom_point()+
         geom_smooth(method=lm,fullrange=TRUE,
                  aes(color=response_type))+
        scale_color_manual(name='Response Type',
                     breaks=c('fast', 'slow'),
                     values=c('large'='#E69F00', 'slow'='#0C7BDC'))+
   theme(legend.title=element_text(size=14),
       legend.text=element_text(size=14))+
         theme_classic()



```

```{r}

# Calculating NND for each responder within the same school and stimulus group
#NAs come from two having the same nearest neighbor distance

nnd_df <- nnd_df %>%
  group_by(school, stimulus) %>%
  mutate(
    NND = sqrt((s_head_x - lag(s_head_x))^2 + (s_head_y - lag(s_head_y))^2)
  ) %>%
  ungroup()

#If we want:
# Replace NA values in the NND column with 0 
#nnd_df$NND[is.na(nnd_df$NND)] <- 0

cym_dat$NND <- nnd_df$NND

#add nnd to wave df
nnd_wave_dat <- merge(wave_dat, nnd_df[, c('school', 'stimulus', 'responder', 'NND')], 
                         by = c('school', 'stimulus', 'responder'), 
                         all.x = TRUE)



```


```{r}

#ANCOVA
ancova_mod <- aov(turning_rate ~ size + response_type, data = cym_dat)
summary(ancova_mod)


par(mfrow=c(2,2))
plot(ancova_mod)

wilcox.test(turning_rate ~ response_type, data = cym_dat)
# p-value <2.2e-16, conclusion: there is a statistically significant difference in the distributions of "turning_rate" between the two response types.


```

```{r}
#Only interested in fast responses, making new dataset excluding slow responses

fast_dat <- cym_dat %>%
  filter(response_type == "fast")

#glm for fast
mod_fast <- glm(latency_ms ~ s_w_reactor + distance_from_stimulus + size+ angle_between_fish_and_stimulus + distance_from_first_responder +NND, data = fast_dat, family=poisson, na.action = na.exclude)
summary(mod_fast)
par(mfrow=c(2,2))
plot(mod_fast)

#subsetting just the wave
wave_dat_fast <- subset(fast_dat, substr(responder, 1, 1) == "w")

#glm for wave
glm_wave_fast <- glm(latency_ms ~ size+ angle_between_fish_and_stimulus+distance_from_first_responder +NND, data = wave_dat_fast, family=Gamma(link = "log"), na.action = na.exclude)
summary(glm_wave_fast)
par(mfrow=c(2,2))
plot(glm_wave_fast)


#taking out all insignificant variables
glm_size_distfr <- glm(latency_ms ~ size + distance_from_first_responder , data = wave_dat_fast, family=Gamma(link = "log"))
summary(glm_size_distfr)
par(mfrow=c(2,2))
plot(glm_size_distfr)
#size is more significant

#linear model for just the wave
lm_wavedat_fast <- lm(latency_ms ~ size, data=wave_dat_fast)
summary(lm_wavedat_fast)


#normality
shapiro.test(lm_wavedat_fast$residuals) 

#homogeneity of variances
bartlett.test(latency_ms ~ size, data=wave_dat_fast)

wilcox.test(latency_ms ~ size, data = wave_dat_fast)



```



```{r}
#Correlation and Principal Component Analysis 


#new df with specified columns
wave_fast_forPCA <- wave_dat_fast[, c("latency_ms", "s_w_reactor", "distance_from_stimulus", "size", "angle_between_fish_and_stimulus", "distance_from_first_responder","NND")]


#make small 0 and large 1
wave_fast_forPCA <- wave_fast_forPCA %>%
  mutate(size_ID = ifelse(size == "small", 0, 1))



#use sapply to make wave_dat_fast numeric

wave_fast_new <- wave_fast_forPCA[sapply(wave_fast_forPCA, is.numeric)]

#correlation
wave_corr <- sapply(wave_fast_new[, setdiff(names(wave_fast_new), "latency_ms")], function(x) {
  cor(wave_fast_new$latency_ms, x, method = "pearson")
})
print(wave_corr)



```


```{r}



```


```{r}
#apprently glm with poisson is not good
#doing a mixed effect model for fast responses

#quick visualization 
ggplot(wave_dat_fast) +
  geom_point(aes(x = distance_from_first_responder, y = latency_ms, color = size)) +
  geom_smooth(aes(x = distance_from_first_responder, y = latency_ms, color = size), method = "lm") +
  scale_color_fish_d(option = "Trimma_lantana") 

mix_mod1 <- glmer(latency_ms ~ distance_from_first_responder + (1 | size), data = wave_dat_fast, family = poisson)

summary(mix_mod1)



```


```{r}

#glm
mod1 <- glm(latency_ms ~ s_w_reactor + distance_from_stimulus + size+ angle_between_fish_and_stimulus+distance_from_first_responder, data = cym_dat, family = gaussian, na.action = na.exclude)
summary(mod1)
plot(mod1)

#take out other variables
mod1 <- glm(latency_ms ~ s_w_reactor + distance_from_stimulus + size+ angle_between_fish_and_stimulus+distance_from_first_responder, data = cym_dat, family = gaussian, na.action = na.exclude)
summary(mod1)
plot(mod1)

#basic glm
size_glm <- glm(latency_ms ~ s_w_reactor + distance_from_stimulus + size+ angle_between_fish_and_stimulus+distance_from_first_responder, data = cym_dat, family = gaussian, na.action = na.exclude)
summary(mod1)
plot(mod1)




```

```{r}

# Predict the values using the fitted model
predicted_values <- predict(mod1, type = "response")

# Create a data frame for plotting
plot_data <- data.frame(Observed = cym_dat$latency_ms, Predicted = predicted_values)

#scatterplot
ggplot(plot_data, aes(x = Observed, y = Predicted)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +  # Add a reference line
  labs(x = "Observed", y = "Predicted") +
  theme_minimal()


ggplt <- ggplot(wave_dat,aes(x=distance_from_first_responder,y=latency_ms,color=size))+
         geom_point()+
         theme_classic()

ggplt+geom_smooth(method=lm,fullrange=TRUE,
                  aes(color=size))

ggplt+scale_color_manual(values=c("#E69F00","#0C7BDC"))

ggplt+scale_color_manual(name='Size',
                     breaks=c('large', 'small'),
                     values=c('large'='#E69F00', 'small'='#0C7BDC'))+
   theme(legend.title=element_text(size=14),
       legend.text=element_text(size=14))


```



```{r}

```




```{r}

```